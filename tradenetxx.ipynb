{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithmic Trading: From Technical Analysis to a PyTorch Neural Network (Refined)\n",
        "\n",
        "This notebook walks through a complete workflow for developing and evaluating algorithmic trading strategies. This version includes **refined strategy logic** to ensure more realistic trade signal generation and a **more robust backtesting engine** for accurate performance metrics.\n",
        "\n",
        "**Workflow:**\n",
        "1.  **Refined Strategy Definition:** Create 5 complex, rule-based trading strategies with improved logic.\n",
        "2.  **Data Fetching:** Download historical data for the top 10 NSE stocks.\n",
        "3.  **Backtesting & Comparison:** Systematically backtest the 5 strategies using a state-based backtester and generate performance comparison tables.\n",
        "4.  **Visual Analysis:** Plot strategy signals on a price chart to visually inspect performance.\n",
        "5.  **Final DNN Model:** Train a robust DNN on combined data from multiple stocks and save it for future predictions."
      ],
      "metadata": {
        "id": "intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setup and Installations"
      ],
      "metadata": {
        "id": "setup_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas_ta tvdatafeed scikit-learn -q"
      ],
      "metadata": {
        "id": "pip_installs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pandas_ta as ta\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tvDatafeed import TvDatafeed, Interval\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import joblib # For saving the scaler\n",
        "\n",
        "# Suppress pandas warnings\n",
        "pd.options.mode.chained_assignment = None"
      ],
      "metadata": {
        "id": "imports_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Refined Rule-Based Strategy Definitions\n",
        "\n",
        "These functions have been refined to generate more consistent signals. Logic for S3 (Bollinger Squeeze) and S5 (ADX Trend) has been relaxed to be more adaptive."
      ],
      "metadata": {
        "id": "strategy_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_s1_triple_threat(df):\n",
        "    \"\"\"S1: Triple Threat (Supertrend, EMA, MACD)\"\"\"\n",
        "    df_strat = df.copy()\n",
        "    df_strat.ta.supertrend(length=10, multiplier=3, append=True, col_names=('SUPERT', 'SUPERTd', 'SUPERTl', 'SUPERTs'))\n",
        "    df_strat.ta.ema(length=50, append=True, col_names=('EMA_50'))\n",
        "    df_strat.ta.macd(fast=12, slow=26, signal=9, append=True)\n",
        "    long_cond = (df_strat['SUPERTd'] == 1) & (df_strat['close'] > df_strat['EMA_50']) & (df_strat['MACD_12_26_9'] > df_strat['MACDs_12_26_9']) & (df_strat['MACD_12_26_9'].shift(1) <= df_strat['MACDs_12_26_9'].shift(1))\n",
        "    short_cond = (df_strat['SUPERTd'] == -1) & (df_strat['close'] < df_strat['EMA_50']) & (df_strat['MACD_12_26_9'] < df_strat['MACDs_12_26_9']) & (df_strat['MACD_12_26_9'].shift(1) >= df_strat['MACDs_12_26_9'].shift(1))\n",
        "    df_strat['S1_Signal'] = np.select([long_cond, short_cond], [1, -1], default=0)\n",
        "    return df_strat\n",
        "\n",
        "def add_s2_rsi_peak(df):\n",
        "    \"\"\"S2: RSI Peak/Trough with MACD Filter\"\"\"\n",
        "    df_strat = df.copy()\n",
        "    df_strat.ta.rsi(length=14, append=True, col_names=('RSI_14'))\n",
        "    df_strat.ta.macd(fast=12, slow=26, signal=9, append=True)\n",
        "    long_cond = (df_strat['MACDh_12_26_9'] > 0) & (df_strat['RSI_14'] < 35) & (df_strat['RSI_14'].shift(1) >= 35)\n",
        "    short_cond = (df_strat['MACDh_12_26_9'] < 0) & (df_strat['RSI_14'] > 65) & (df_strat['RSI_14'].shift(1) <= 65)\n",
        "    df_strat['S2_Signal'] = np.select([long_cond, short_cond], [1, -1], default=0)\n",
        "    return df_strat\n",
        "\n",
        "def add_s3_bollinger_squeeze(df):\n",
        "    \"\"\"S3: Bollinger Band Squeeze Breakout (Refined)\"\"\"\n",
        "    df_strat = df.copy()\n",
        "    bbands = df_strat.ta.bbands(length=20, std=2)\n",
        "    if bbands is not None and not bbands.empty:\n",
        "      df_strat = df_strat.join(bbands)\n",
        "      if 'BBB_20_2.0' in df_strat.columns:\n",
        "        # REFINED LOGIC: Squeeze is when width is below its moving average\n",
        "        squeeze_cond = df_strat['BBB_20_2.0'] < df_strat['BBB_20_2.0'].rolling(window=120).mean()\n",
        "        volume_cond = df_strat['volume'] > (df_strat['volume'].rolling(window=20).mean() * 1.5)\n",
        "        long_cond = squeeze_cond & (df_strat['close'] > df_strat['BBU_20_2.0']) & volume_cond\n",
        "        short_cond = squeeze_cond & (df_strat['close'] < df_strat['BBL_20_2.0']) & volume_cond\n",
        "        df_strat['S3_Signal'] = np.select([long_cond, short_cond], [1, -1], default=0)\n",
        "      else:\n",
        "        df_strat['S3_Signal'] = 0\n",
        "    else:\n",
        "      df_strat['S3_Signal'] = 0\n",
        "    return df_strat\n",
        "\n",
        "def add_s4_ema_stoch(df):\n",
        "    \"\"\"S4: Multi-Timeframe EMA with Stochastic Entry\"\"\"\n",
        "    df_strat = df.copy()\n",
        "    df_strat.ta.ema(length=20, append=True, col_names=('EMA_20'))\n",
        "    df_strat.ta.ema(length=50, append=True, col_names=('EMA_50'))\n",
        "    df_strat.ta.ema(length=100, append=True, col_names=('EMA_100'))\n",
        "    stoch = df_strat.ta.stoch(k=14, d=3, smooth_k=3)\n",
        "    if stoch is not None and not stoch.empty:\n",
        "        df_strat = df_strat.join(stoch)\n",
        "        if 'STOCHk_14_3_3' in df_strat.columns:\n",
        "          long_trend = (df_strat['EMA_20'] > df_strat['EMA_50']) & (df_strat['EMA_50'] > df_strat['EMA_100'])\n",
        "          short_trend = (df_strat['EMA_20'] < df_strat['EMA_50']) & (df_strat['EMA_50'] < df_strat['EMA_100'])\n",
        "          long_entry = (df_strat['STOCHk_14_3_3'] < 20) & (df_strat['STOCHk_14_3_3'] > df_strat['STOCHd_14_3_3']) & (df_strat['STOCHk_14_3_3'].shift(1) <= df_strat['STOCHd_14_3_3'].shift(1))\n",
        "          short_entry = (df_strat['STOCHk_14_3_3'] > 80) & (df_strat['STOCHk_14_3_3'] < df_strat['STOCHd_14_3_3']) & (df_strat['STOCHk_14_3_3'].shift(1) >= df_strat['STOCHd_14_3_3'].shift(1))\n",
        "          df_strat['S4_Signal'] = np.select([long_trend & long_entry, short_trend & short_entry], [1, -1], default=0)\n",
        "        else:\n",
        "          df_strat['S4_Signal'] = 0\n",
        "    else:\n",
        "        df_strat['S4_Signal'] = 0\n",
        "    return df_strat\n",
        "\n",
        "def add_s5_adx_trend(df):\n",
        "    \"\"\"S5: ADX + Keltner Channel Trend Strategy (Refined)\"\"\"\n",
        "    df_strat = df.copy()\n",
        "    df_strat.ta.adx(length=14, append=True)\n",
        "    kc = df_strat.ta.kc(length=20, scalar=2)\n",
        "    if kc is not None and not kc.empty:\n",
        "        df_strat = df_strat.join(kc)\n",
        "        if 'ADX_14' in df_strat.columns and 'KCu_20_2' in df_strat.columns:\n",
        "          # REFINED LOGIC: Trigger on strong OR developing trends\n",
        "          strong_trend = (df_strat['ADX_14'] > 25) | ((df_strat['ADX_14'] > 20) & (df_strat['ADX_14'] > df_strat['ADX_14'].shift(1)))\n",
        "          long_cond = strong_trend & (df_strat['close'] > df_strat['KCu_20_2']) & (df_strat['close'].shift(1) <= df_strat['KCu_20_2'].shift(1))\n",
        "          short_cond = strong_trend & (df_strat['close'] < df_strat['KCl_20_2']) & (df_strat['close'].shift(1) >= df_strat['KCl_20_2'].shift(1))\n",
        "          df_strat['S5_Signal'] = np.select([long_cond, short_cond], [1, -1], default=0)\n",
        "        else:\n",
        "          df_strat['S5_Signal'] = 0\n",
        "    else:\n",
        "        df_strat['S5_Signal'] = 0\n",
        "    return df_strat"
      ],
      "metadata": {
        "id": "strategy_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Data Fetching"
      ],
      "metadata": {
        "id": "fetching_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tv = TvDatafeed()\n",
        "top_stocks_nse = [\n",
        "    \"RELIANCE\", \"TCS\", \"HDFCBANK\", \"ICICIBANK\", \"INFY\", \n",
        "    \"HINDUNILVR\", \"SBIN\", \"BHARTIARTL\", \"ITC\", \"LT\"\n",
        "]\n",
        "stock_dataframes = {}\n",
        "\n",
        "print(\"Fetching 500 days of daily data for top NSE stocks...\")\n",
        "for stock_symbol in top_stocks_nse:\n",
        "    try:\n",
        "        data = tv.get_hist(\n",
        "            symbol=stock_symbol, exchange=\"NSE\", \n",
        "            interval=Interval.in_daily, n_bars=500\n",
        "        )\n",
        "        stock_dataframes[stock_symbol] = data\n",
        "        print(f\"Successfully fetched data for {stock_symbol}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not fetch data for {stock_symbol}: {e}\")\n",
        "\n",
        "print(\"\\nData fetching complete.\")"
      ],
      "metadata": {
        "id": "fetching_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Backtesting and Comparing Rule-Based Strategies\n",
        "\n",
        "This section uses the **refined backtesting loop** to ensure accurate trade counts and performance metrics."
      ],
      "metadata": {
        "id": "backtesting_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_performance_metrics(trades_df, stock_symbol, strat_name, risk_free_rate=0.07):\n",
        "    \"\"\"Calculates performance metrics and returns them as a dictionary.\"\"\"\n",
        "    if trades_df.empty or len(trades_df) < 2:\n",
        "        return {'Stock': stock_symbol, 'Strategy': strat_name, 'Sharpe Ratio': np.nan, 'Avg Return (%)': np.nan, 'Strike Rate (%)': np.nan, 'Num Trades': 0}\n",
        "\n",
        "    num_trades = len(trades_df)\n",
        "    avg_holding_period = trades_df['holding_days'].mean()\n",
        "    profitable_trades = trades_df[trades_df['return'] > 0]\n",
        "    strike_rate = (len(profitable_trades) / num_trades) * 100\n",
        "    avg_return_pct = trades_df['return'].mean()\n",
        "\n",
        "    trade_returns = trades_df['return']\n",
        "    sharpe_ratio = np.nan\n",
        "    if trade_returns.std() > 0 and avg_holding_period > 0:\n",
        "        annualization_factor = np.sqrt(252 / avg_holding_period)\n",
        "        avg_excess_return = trade_returns.mean() * (252 / avg_holding_period) - risk_free_rate\n",
        "        annualized_volatility = trade_returns.std() * annualization_factor\n",
        "        if annualized_volatility != 0:\n",
        "            sharpe_ratio = avg_excess_return / annualized_volatility\n",
        "\n",
        "    return {\n",
        "        'Stock': stock_symbol,\n",
        "        'Strategy': strat_name,\n",
        "        'Sharpe Ratio': sharpe_ratio,\n",
        "        'Avg Return (%)': avg_return_pct * 100,\n",
        "        'Strike Rate (%)': strike_rate,\n",
        "        'Num Trades': num_trades\n",
        "    }\n",
        "\n",
        "print(\"Backtesting all rule-based strategies...\")\n",
        "all_results = []\n",
        "strategies = {\n",
        "    \"S1_TripleThreat\": add_s1_triple_threat,\n",
        "    \"S2_RsiPeak\": add_s2_rsi_peak,\n",
        "    \"S3_BollingerSqueeze\": add_s3_bollinger_squeeze,\n",
        "    \"S4_EmaStoch\": add_s4_ema_stoch,\n",
        "    \"S5_AdxTrend\": add_s5_adx_trend\n",
        "}\n",
        "\n",
        "dataframes_with_signals = {}\n",
        "\n",
        "for stock_symbol, df in stock_dataframes.items():\n",
        "    df_with_all_signals = df.copy()\n",
        "    for strat_name, strat_func in strategies.items():\n",
        "        df_with_signals = strat_func(df)\n",
        "        signal_col_name = strat_name.split('_')[0] + '_Signal'\n",
        "        df_with_all_signals[signal_col_name] = df_with_signals[signal_col_name]\n",
        "        \n",
        "        # --- REFINED STATE-BASED BACKTESTING LOOP ---\n",
        "        trade_log = []\n",
        "        position = 0 # 0: flat, 1: long, -1: short\n",
        "        entry_price = 0\n",
        "        entry_date = None\n",
        "\n",
        "        for date, row in df_with_signals.iterrows():\n",
        "            signal = row[signal_col_name]\n",
        "            \n",
        "            # Case 1: In a LONG position, check for exit\n",
        "            if position == 1 and signal == -1:\n",
        "                exit_price = row['close']\n",
        "                trade_return = (exit_price - entry_price) / entry_price\n",
        "                holding_days = (date - entry_date).days\n",
        "                if holding_days > 0: trade_log.append({'return': trade_return, 'holding_days': holding_days})\n",
        "                position = 0 # Exit position\n",
        "            \n",
        "            # Case 2: In a SHORT position, check for exit\n",
        "            elif position == -1 and signal == 1:\n",
        "                exit_price = row['close']\n",
        "                trade_return = ((entry_price - exit_price) / entry_price)\n",
        "                holding_days = (date - entry_date).days\n",
        "                if holding_days > 0: trade_log.append({'return': trade_return, 'holding_days': holding_days})\n",
        "                position = 0 # Exit position\n",
        "            \n",
        "            # Case 3: NOT in a position, check for entry\n",
        "            if position == 0 and signal != 0:\n",
        "                position = signal\n",
        "                entry_price = row['close']\n",
        "                entry_date = date\n",
        "\n",
        "        trades_df = pd.DataFrame(trade_log)\n",
        "        metrics = get_performance_metrics(trades_df, stock_symbol, strat_name)\n",
        "        all_results.append(metrics)\n",
        "    dataframes_with_signals[stock_symbol] = df_with_all_signals\n",
        "\n",
        "print(\"Backtesting complete.\")\n"
      ],
      "metadata": {
        "id": "backtesting_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4a: Visualizing Strategy Signals on RELIANCE\n",
        "\n",
        "Before looking at the statistical tables, let's visualize how each strategy's signals look on the price chart for a single stock. We will use the last 200 days of data for RELIANCE."
      ],
      "metadata": {
        "id": "plotting_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_strategy_signals(df, strat_name, signal_col):\n",
        "    \"\"\"Plots the close price and buy/sell signals for a given strategy.\"\"\"\n",
        "    plot_df = df.tail(200).copy()\n",
        "    \n",
        "    buy_signals = plot_df[plot_df[signal_col] == 1]\n",
        "    sell_signals = plot_df[plot_df[signal_col] == -1]\n",
        "    \n",
        "    plt.figure(figsize=(15, 7))\n",
        "    plt.plot(plot_df.index, plot_df['close'], label='Close Price', color='skyblue', linewidth=2)\n",
        "    \n",
        "    plt.scatter(buy_signals.index, buy_signals['close'], label='Buy Signal', marker='^', color='green', s=150, zorder=5)\n",
        "    plt.scatter(sell_signals.index, sell_signals['close'], label='Sell Signal', marker='v', color='red', s=150, zorder=5)\n",
        "    \n",
        "    plt.title(f'{strat_name} Strategy Signals on RELIANCE (Last 200 Days)')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Price')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "print(\"Generating strategy performance plots for RELIANCE...\")\n",
        "reliance_df_with_signals = dataframes_with_signals.get(\"RELIANCE\")\n",
        "if reliance_df_with_signals is not None:\n",
        "    for strat_name in strategies.keys():\n",
        "        plot_strategy_signals(reliance_df_with_signals, strat_name, strat_name.split('_')[0] + '_Signal')\n"
      ],
      "metadata": {
        "id": "plotting_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4b: Final Performance Comparison Tables"
      ],
      "metadata": {
        "id": "tables_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "sharpe_pivot = results_df.pivot_table(index='Strategy', columns='Stock', values='Sharpe Ratio')\n",
        "sharpe_pivot['Average'] = sharpe_pivot.mean(axis=1)\n",
        "print(\"--- Comparison Table: Sharpe Ratio ---\")\n",
        "print(sharpe_pivot.round(2))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "avg_return_pivot = results_df.pivot_table(index='Strategy', columns='Stock', values='Avg Return (%)')\n",
        "avg_return_pivot['Average'] = avg_return_pivot.mean(axis=1)\n",
        "print(\"--- Comparison Table: Average Return per Trade (%) ---\")\n",
        "print(avg_return_pivot.round(2))"
      ],
      "metadata": {
        "id": "tables_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Neural Network Implementation (Single Stock Demo)\n",
        "\n",
        "This section builds the \"Expert Committee\" model on a single stock (RELIANCE) for a clear demonstration."
      ],
      "metadata": {
        "id": "nn_markdown_single_stock"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preparing data for the Neural Network...\")\n",
        "\n",
        "# Select one stock for the NN demonstration\n",
        "symbol = \"RELIANCE\"\n",
        "# Use the dataframe that already has all signals calculated\n",
        "nn_df = dataframes_with_signals[symbol].copy()\n",
        "\n",
        "# 1. Engineer raw features\n",
        "nn_df.ta.rsi(length=14, append=True, col_names=('RSI'))\n",
        "nn_df.ta.macd(fast=12, slow=26, signal=9, append=True)\n",
        "nn_df.ta.atr(length=14, append=True, col_names=('ATR'))\n",
        "stoch = nn_df.ta.stoch(k=14, d=3, smooth_k=3)\n",
        "if stoch is not None and not stoch.empty: nn_df = nn_df.join(stoch)\n",
        "\n",
        "# 2. Create the combined feature set (X)\n",
        "features = [\n",
        "    'RSI', 'MACD_12_26_9', 'MACDh_12_26_9', 'ATR', 'STOCHk_14_3_3',\n",
        "    'S1_Signal', 'S2_Signal', 'S3_Signal', \n",
        "    'S4_Signal', 'S5_Signal'\n",
        "]\n",
        "\n",
        "# 3. Create the multi-class target (y)\n",
        "threshold = 0.0075  # 0.75% move for a Long/Short signal\n",
        "conditions = [\n",
        "    (nn_df['close'].shift(-1) > nn_df['close'] * (1 + threshold)),\n",
        "    (nn_df['close'].shift(-1) < nn_df['close'] * (1 - threshold))\n",
        "]\n",
        "choices = [2, 0]  # 2 for Long, 0 for Short\n",
        "nn_df['y'] = np.select(conditions, choices, default=1) # 1 for Hold\n",
        "\n",
        "# 4. Prepare data for PyTorch\n",
        "nn_df.dropna(inplace=True)\n",
        "X = nn_df[features]\n",
        "y = nn_df['y'].values\n",
        "\n",
        "X = X[:-1]; y = y[:-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "print(\"Data preparation complete.\")"
      ],
      "metadata": {
        "id": "nn_data_prep_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Define the Neural Network Model\n",
        "class MultiClassNN(nn.Module):\n",
        "    def __init__(self, input_features, num_classes):\n",
        "        super(MultiClassNN, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_features, 128)\n",
        "        self.layer2 = nn.Linear(128, 64)\n",
        "        self.output_layer = nn.Linear(64, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "# 6. Train the Model\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "num_classes = 3 # (Short, Hold, Long)\n",
        "model = MultiClassNN(input_features=input_dim, num_classes=num_classes)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"\\nStarting model training...\")\n",
        "epochs = 300\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# 7. Evaluate the Model\n",
        "print(\"\\nEvaluating model on test data...\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    _, predicted_signals = torch.max(test_outputs, 1)\n",
        "    \n",
        "    print(f'\\nModel Accuracy on Test Data: {accuracy_score(y_test_tensor.numpy(), predicted_signals.numpy()) * 100:.2f}%')\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test_tensor.numpy(), predicted_signals.numpy(), target_names=['Short (0)', 'Hold (1)', 'Long (2)']))"
      ],
      "metadata": {
        "id": "nn_train_eval_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Final DNN Model with Combined Data\n",
        "\n",
        "This is the final, most robust model. It's trained on a large, combined dataset from the top 5 stocks to learn more generalized patterns. The features are the signals from our 5 rule-based strategies. The trained model and scaler are saved so they can be loaded later to make predictions on new data."
      ],
      "metadata": {
        "id": "final_dnn_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Building Final DNN Model on Combined Data ---\")\n",
        "\n",
        "# 1. Fetch larger dataset for top 5 stocks\n",
        "top_5_stocks = [\"RELIANCE\", \"TCS\", \"HDFCBANK\", \"ICICIBANK\", \"INFY\"]\n",
        "combined_df_list = []\n",
        "\n",
        "print(\"\\nFetching 2500 bars for each of the top 5 stocks...\")\n",
        "for symbol in top_5_stocks:\n",
        "    try:\n",
        "        data = tv.get_hist(symbol=symbol, exchange=\"NSE\", interval=Interval.in_daily, n_bars=2500)\n",
        "        print(f\"Fetched data for {symbol}\")\n",
        "        \n",
        "        # 2. Add all strategy signals\n",
        "        data = add_s1_triple_threat(data)\n",
        "        data = add_s2_rsi_peak(data)\n",
        "        data = add_s3_bollinger_squeeze(data)\n",
        "        data = add_s4_ema_stoch(data)\n",
        "        data = add_s5_adx_trend(data)\n",
        "        \n",
        "        # 3. Create multi-class target variable\n",
        "        threshold = 0.0075\n",
        "        conditions = [\n",
        "            (data['close'].shift(-1) > data['close'] * (1 + threshold)),\n",
        "            (data['close'].shift(-1) < data['close'] * (1 - threshold))\n",
        "        ]\n",
        "        choices = [2, 0] # Long, Short\n",
        "        data['y'] = np.select(conditions, choices, default=1) # Hold\n",
        "        \n",
        "        combined_df_list.append(data)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not process {symbol}: {e}\")\n",
        "\n",
        "# 4. Combine all dataframes into one\n",
        "final_df = pd.concat(combined_df_list)\n",
        "final_df.dropna(inplace=True)\n",
        "\n",
        "print(\"\\nCombined dataset created.\")\n",
        "\n",
        "# 5. Prepare data for PyTorch\n",
        "features = ['S1_Signal', 'S2_Signal', 'S3_Signal', 'S4_Signal', 'S5_Signal']\n",
        "X = final_df[features]\n",
        "y = final_df['y'].values\n",
        "\n",
        "# Split into 80% train, 20% test\n",
        "split_index = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "print(\"Data prepared for final DNN.\")\n",
        "\n",
        "# 6. Define and Train the Final DNN\n",
        "class FinalDNN(nn.Module):\n",
        "    def __init__(self, input_features, num_classes):\n",
        "        super(FinalDNN, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_features, 64)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.output_layer = nn.Linear(32, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "final_model = FinalDNN(input_features=input_dim, num_classes=3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(final_model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"\\nStarting final model training...\")\n",
        "epochs = 400 # Train for more epochs on the larger dataset\n",
        "for epoch in range(epochs):\n",
        "    final_model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = final_model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# 7. Final Model Evaluation (Rating)\n",
        "print(\"\\n--- Final Model Rating ---\")\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = final_model(X_test_tensor)\n",
        "    _, predicted_signals = torch.max(test_outputs, 1)\n",
        "    print(f'\\nModel Accuracy on Unseen Test Data: {accuracy_score(y_test_tensor.numpy(), predicted_signals.numpy()) * 100:.2f}%')\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test_tensor.numpy(), predicted_signals.numpy(), target_names=['Short (0)', 'Hold (1)', 'Long (2)']))\n",
        "\n",
        "# 8. Save the Model and Scaler for Future Use\n",
        "torch.save(final_model.state_dict(), 'final_dnn_model.pth')\n",
        "joblib.dump(scaler, 'final_dnn_scaler.gz')\n",
        "print(\"\\nFinal model and data scaler have been saved.\")\n",
        "print(\"You can now load 'final_dnn_model.pth' and 'final_dnn_scaler.gz' to make predictions on new data.\")"
      ],
      "metadata": {
        "id": "final_dnn_code"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
